# In the proposed methodology, $N$ Scenarios are generated in the following steps.
# 1. Generate Halton sequence (quasi Monte Carlo samples) of sufficient length, say $200N$, from the normal distribution $\mathcal{N}(0,I)$.
# 2. Perform $k$-means clustering to find $N$ centroids of the $200N$ samples, using Lloyd's algorithm, for example.
# 3. Dilate these samples, so that tails are covered-well, up to $m$ standard deviations, for example $m = 4$. These centroids are scattered to cover the distribution $\mathcal{N}(0,mI)$ well, effectively representing the tail-regions of $\mathcal{N}(0,I)$ with the given number $N$ of samples.
# 4. Assign probability weights for each of these samples, by using the Voronoi diagram generated by $k$-means clustering in step 2.

from sklearn import cluster
from sklearn.neighbors import NearestNeighbors
from scipy.stats import qmc
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.special import erfinv
import pandas as pd


# --- general helper constants and functions ------------------------------------------------------

# set scenario directories
SCENARIO_DIR = os.environ.get("SCENARIO_DIR", "scenario")
os.makedirs(SCENARIO_DIR, exist_ok=True)

# set figure output directory
FIGURE_DIR = os.environ.get("SCENARIO_OUTPUTS", "scenario_outputs")
os.makedirs(FIGURE_DIR, exist_ok=True)

SIGMA_0 = 4.0                   # scale factor for standard normal
EPS = 1e-12                     # numerical tolerance

# parameters for Halton sequence
NDIMS = 20                      # number of dimensions
NSAMPLES = 1000                 # number of samples

# Hybrid selection config
K_ANCHORS           = 60        # number of medoid anchors (tune as needed)
SEED                = 42        # random seed for reproducibility, use in medoids
NEVAL_PTS           = 1000000   # number of evaluation points for probability weights
EXPAND_MODE         = "diverse" # "none" | "rnn" | "diverse" | "tail"
R_PER_CLUSTER       = 1         # extras per cluster if EXPAND_MODE != "none"
TAIL_QUANTILE       = 0.95      # if EXPAND_MODE == "tail"
K_INIT              = 60        # initial number of medoids
TARGET_THRESHOLD    = 2.0       # target threshold for refinement


# overwrite-safe save (mirror your current pattern)
def save_scenario(info, path, arr):
    filename = os.path.basename(path)
    full_path = os.path.join(SCENARIO_DIR, filename)
    base, ext = os.path.splitext(full_path)
    out = full_path if not os.path.exists(full_path) else f"{base}_new{ext}"
    np.savetxt(out, arr)
    print(info)

    return out

# save figure in certain directory
def savefigp(info="[INFO]:", fname='fig.png'):
    """Save figure as png in the output directory."""
    # Create output directory
    subdir = FIGURE_DIR
    fname = os.path.join(subdir, fname)
    plt.savefig(fname, dpi=250, bbox_inches='tight')
    print(f"{info} > File name: {fname}")
    plt.close()

# end of general helper constants and functions ---------------------------------------------------



# (1) --- generate Halton points ------------------------------------------------------------------

# 1.1 halton generator
def halton_generator(ndims=NDIMS, nsamples=NSAMPLES, sigma=SIGMA_0):
    """Generate Halton sequence points in [0,1]^d and map to standard normal via inverse CDF."""

    # helper function for the the inv error-function
    def icdf(x):
        return erfinv(2.*x - 1.)

    # create Halton sequence sampler
    sequencer = qmc.Halton(d=ndims, scramble=False)                  
    # fast-forward to skip first points with poor uniformity
    sequencer.fast_forward(12345)
    halton_std_hpts = np.array(sequencer.random(n=nsamples))

    # map Halton points to standard normal via inverse CDF
    halton_std_pts = icdf(halton_std_hpts)

    # uniform weights for Halton samples in standard-normal space
    halton_std_wgts = np.full(nsamples, 1.0 / float(nsamples))

    halton = pd.DataFrame({
        # store per-sample vectors as list entries so each column is 1-D
        'halton_std_pts'    : list(halton_std_pts),
        'halton_std_hpts'   : list(halton_std_hpts),
        'halton_pts'        : list(halton_std_pts*sigma),
        # per-sample weights (1D array) kept as a regular column
        'halton_std_wgts'   : halton_std_wgts,
    })

    return halton

# 1.2. plot Halton distributions 
def plot_halton_distributions(halton):
    """Plot Halton points: original in [0,1]^2 and standard normal mapped."""

    # stored as lists of per-sample vectors in DataFrame; convert back to 2D arrays
    halton_std_hpts = np.asarray(list(halton['halton_std_hpts']))
    halton_std_pts  = np.asarray(list(halton['halton_std_pts']))
    
    XMAX = 6.0      # limit for x-axis 
    YMAX = 6.0      # limit for y-axis
    # Figure setup with two subplots (side by side)
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))
    # ---- Figure #1 ----
    ax = axes[0]
    ax.plot(halton_std_hpts[:, 0], halton_std_hpts[:, 1], '.', 
            markersize=2, label='Halton points (original)')
    ax.set_xlabel('$z_1$')
    ax.set_ylabel('$z_2$')
    ax.legend()
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)
    ax.set_xlim([-0.1, 1.1])
    ax.set_ylim([-0.1, 1.1])

    # ---- Figure #2 ----
    theta = np.linspace(0, 2*np.pi, 200)
    ct, st = np.cos(theta), np.sin(theta)

    ax = axes[1]
    ax.plot(ct, st, '-k', linewidth=0.5, alpha=0.5)
    for k in range(int(XMAX)):
        ax.plot(k * ct, k * st, '-k', linewidth=0.5, alpha=0.2)
    ax.plot([-XMAX-4, XMAX+4], [0, 0],'k', linewidth=0.5)
    ax.plot([0, 0], [-YMAX-4, YMAX+4], 'k', linewidth=0.5)

    ax.plot(halton_std_pts[:, 0], halton_std_pts[:, 1], '.', markersize=5, label='Halton points (standard normal)')
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)
    ax.set_xlim([-XMAX, XMAX])
    ax.set_ylim([-YMAX, YMAX])
    ax.set_xlabel('$z_1$')
    ax.set_ylabel('$z_2$')
    ax.legend()

    # Adjust layout for neat appearance
    plt.tight_layout()

    info = "[INFO 012]: > Save halton points"
    savefigp(info, '012_halton_points.png')

# 1.3. plot scaled Halton points
def plot_scaled_halton(halton, sigma):
    """Scaled Halton points with standard deviation (4.0)"""
    
    halton_std_pts = np.asarray(list(halton['halton_std_pts']))
    
    XMAX = 6.0      # limit for x-axis 
    YMAX = 6.0      # limit for y-axis
    EXTD = 4

    theta = np.linspace(0, 2*np.pi, 200)
    ct, st = np.cos(theta), np.sin(theta)

    fig, ax = plt.subplots(figsize=(5,5))
    for k in range(int(XMAX)+4):
        plt.plot(k*ct,k*st,'-k',linewidth=0.5, alpha=0.2)
    ax.plot(sigma*halton_std_pts[:,0],sigma*halton_std_pts[:,1],'.',
            markersize=5, label=f"$sigma$={sigma}")
    ax.plot([-XMAX-4, XMAX+4], [0, 0],'k', linewidth=0.5)
    ax.plot([0, 0], [-YMAX-4, YMAX+4], 'k', linewidth=0.5)
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)
    tick_values = [-8, -4, 0, 4, 8]
    ax.set_xticks(tick_values)
    ax.set_xticklabels([str(v) for v in tick_values])
    ax.set_yticks(tick_values)
    ax.set_yticklabels([str(v) for v in tick_values])
    ax.set_xlim(-XMAX-EXTD, XMAX+EXTD)
    ax.set_ylim(-YMAX-EXTD, YMAX+EXTD)
    ax.set_xlabel('$z_1$')
    ax.set_ylabel('$z_2$')
    ax.legend(loc='upper right')

    info = "[INFO 013]: > Save scaled halton points"
    savefigp(info, '013_halton_points_scaled.png')

# 1.4. k-means clustering
def kmeans_clustering(halton, N_SAMPLES=100, sigma0=SIGMA_0, ndim=100):
    """"kmeans clustering to generate scenario """
    # convert list-of-vectors back to 2D array for computation
    halton_std_pts = np.asarray(list(halton['halton_std_pts']))
    fitpts = sigma0 * halton_std_pts

    kmeans_model = cluster.KMeans(n_clusters=100, random_state=1).fit(fitpts)
    kmeans_pts = kmeans_model.cluster_centers_
    
    kmeans_wgts = np.zeros(N_SAMPLES)
    NEVAL_PTS = 1500000

    eval_pts = np.random.randn(NEVAL_PTS,20)
    ii,unordered_wgts = np.unique(kmeans_model.predict(eval_pts), return_counts=True)
    unordered_wgts = unordered_wgts/ float(unordered_wgts.sum())

    for k,ic in enumerate(ii):
        kmeans_wgts[ic] = unordered_wgts[k]
    
    info = "[INFO 014]: + save scenarios for KMEANS"
    save_scenario(info, "kmeans_pts.txt", kmeans_pts)
    save_scenario(info, "kmeans_prb_wgts.txt", kmeans_wgts)

    kmeans = pd.DataFrame({
        # store per-cluster vectors as list entries so each column is 1-D
        'kmeans_pts'    : list(kmeans_pts),
        'kmeans_wgts'   : kmeans_wgts,
    })
    kmeans.describe().T


    return kmeans

# (2) --- Medoid cluster method ------------------------------------------------------------------------

# Medoids helper function
def pairwise_dist(X, Y):
    X = np.asarray(X, dtype=float)
    Y = np.asarray(Y, dtype=float)
    out = np.empty((X.shape[0], Y.shape[0]), dtype=float)
    for i in range(X.shape[0]):
        diff = Y - X[i]
        out[i] = np.sqrt(np.sum(diff * diff, axis=1))
    return out

# Symmetric percentage error (bounded between 0 and 200)
def smape_percent(a, b):
    a = np.asarray(a, dtype=float)
    b = np.asarray(b, dtype=float)
    return 200.0 * np.abs(a - b) / (np.abs(a) + np.abs(b) + EPS)

# Pure python weighted k-medoids
def weighted_kmedoids(pts, wgts, k=20, max_iter=100, seed=42):
    """
    Pure python weigthed medoids clustering
    input can be: general halton sequence or monte carlo samples
    Its good to compare with scipy-extra medoids 
    """
    pts = np.asarray(pts, dtype=float)
    if pts.ndim != 2:
        raise ValueError('pts must be a 2D array')

    wgts = np.asarray(wgts, dtype=float).reshape(-1)
    N = pts.shape[0]
    if wgts.size != N:
        raise ValueError('wgts must have the same length as pts')
    if np.any(wgts < 0):
        raise ValueError('wgts must be non-negative')

    weight_sum = wgts.sum()
    if not np.isfinite(weight_sum) or weight_sum <= 0:
        raise ValueError('wgts must sum to a positive finite value')

    rng = np.random.default_rng(seed)
    base_prob = wgts / weight_sum

    medoid_indices = [rng.choice(N, p=base_prob)]
    for _ in range(1, k):
        current = pts[medoid_indices]
        d = pairwise_dist(pts, current)
        nearest_d = d.min(axis=1)
        scaled = nearest_d / (nearest_d.sum() + 1e-12)
        probs = wgts * scaled
        probs_sum = probs.sum()
        if probs_sum <= 0 or not np.isfinite(probs_sum):
            probs = base_prob
        else:
            probs = probs / probs_sum
        medoid_indices.append(rng.choice(N, p=probs))
    medoid_indices = np.array(medoid_indices, dtype=int)

    for _ in range(max_iter):
        medoids = pts[medoid_indices]
        D_to_medoids = pairwise_dist(pts, medoids)
        labels = D_to_medoids.argmin(axis=1)
        changed = False
        for m_idx in range(k):
            members = np.where(labels == m_idx)[0]
            if members.size == 0:
                continue
            subD = pairwise_dist(pts[members], pts[members])
            w_sub = wgts[members]
            if np.all(w_sub == 0):
                candidate_costs = subD.sum(axis=1)
            else:
                candidate_costs = subD.T @ w_sub
            best_local_idx = members[np.argmin(candidate_costs)]
            if best_local_idx != medoid_indices[m_idx]:
                medoid_indices[m_idx] = best_local_idx
                changed = True
        if not changed:
            break

    medoids = pts[medoid_indices]
    D_to_medoids = pairwise_dist(pts, medoids)
    labels = D_to_medoids.argmin(axis=1)
    cluster_wgts = np.array([wgts[labels == i].sum() for i in range(k)])
    dmin = np.min(D_to_medoids, axis=1)

    return medoid_indices, medoids, labels, cluster_wgts, dmin


# 2.1. K-Medoids anchors (main function)
def medoid_anchors(halton, k_anchors, seed):
    """ Select K_ANCHORS medoids from halton_pts using weighted_kmedoids with uniform weights. """
    # We reuse your weighted_kmedoids. If you have “must-run” extremes, add them to preselected_idx so they’re always included.
    
    # --- run pure medoids (anchor)
    # convert stored list-of-vectors into 2D array for medoids computation
    halton_pts = np.asarray(list(halton['halton_pts']))
    n = halton_pts.shape[0]

    # Optional: pre-lock extremes here,  we can set predefine indices here, if available:
    preselected_idx = np.array([], dtype=int)  # e.g., np.array([0, 123, 777])
    K_eff = max(k_anchors - len(preselected_idx), 0)

    # Uniform weights for anchor discovery to avoid over-fitting mass (just for medoids)
    halton_std_wgts = np.full(n, 1.0 / n)

    if K_eff > 0:
        # Your existing routine (keep it!)
        medoid_idx, *_ = weighted_kmedoids(halton_pts, halton_std_wgts, k=K_eff, seed=seed)
        medoid_idx = np.unique(np.concatenate([preselected_idx, medoid_idx])).astype(int)
    else:
        medoid_idx = preselected_idx.copy()

    print("[INFO 021]: Generating Medoid Anchors")
    print("[INFO 021]: Warning! We use uniform weights to avoid over-fitting mass!")
    print(f"[INFO 021]: Pure Medoids only (Anchors): {len(medoid_idx)}")

    return medoid_idx


# (3) --- Nearest Neighbor (NN) Clustering Methods ------------------------------------------------

# --- NN helper function

# refine_until_threshold function
def refine_until_threshold(pts, initial_indices, threshold):
    pts = np.asarray(pts, dtype=float)
    S = list(initial_indices)
    if len(S) == 0:
        raise ValueError('initial_indices must contain at least one index')
    dmin = np.min(pairwise_dist(pts, pts[S]), axis=1)
    max_dists = [dmin.max()]
    while dmin.max() > threshold and len(S) < pts.shape[0]:
        i = int(np.argmax(dmin))
        S.append(i)
        dmin = np.minimum(dmin, np.linalg.norm(pts - pts[i], axis=1))
        max_dists.append(dmin.max())
    return np.array(S, dtype=int), dmin, max_dists

# find_elbow function
def find_elbow(max_dists):
    values = np.asarray(max_dists, dtype=float)
    if values.size == 0:
        raise ValueError('max_dists must contain at least one value')
    if values.ndim > 1:
        reduce_axes = tuple(range(1, values.ndim))
        values = values.max(axis=reduce_axes)
    values = values.reshape(-1)
    n = values.size
    if n == 1:
        return 0, float(values[0])
    xs = np.arange(n, dtype=float)
    XMAX, YMAX = xs[0], values[0]
    x1, y1 = xs[-1], values[-1]
    denom = np.hypot(x1 - XMAX, y1 - YMAX)
    if denom == 0:
        return 0, float(values[0])
    d = np.abs((y1 - YMAX) * xs - (x1 - XMAX) * values + x1 * YMAX - y1 * XMAX) / denom
    elbow_idx = int(np.argmax(d))
    return elbow_idx, float(values[elbow_idx])

# r-nearest neighbors
def pick_rnn(X, idx, base_idx, r):
    """ r-nearest neighbors (by your pairwise_dist) around base_idx, excluding base itself. """
    if r <= 0 or len(idx) <= 1:
        return []
    # distances from each member to base
    d = pairwise_dist(X[idx], X[[base_idx]]).ravel()
    order = np.argsort(d)
    out = []
    for k in order:
        if idx[k] == base_idx:
            continue
        out.append(idx[k])
        if len(out) >= r:
            break
    return out

# farthest-point inside the cluster
def pick_diverse_farthest(X, idx, base_idx, r):
    """ Greedy farthest-point inside the cluster (for diversity coverage). """
    if r <= 0:
        return []
    selected = [base_idx]
    cand = set(idx) - {base_idx}
    while len(selected) - 1 < r and cand:
        S = np.array(selected, int)
        # min distance to current selected set
        dmin = np.min(pairwise_dist(X[list(cand)], X[S]), axis=1)
        j = int(np.argmax(dmin))
        choice = list(cand)[j]
        selected.append(choice)
        cand.remove(choice)
    return [s for s in selected if s != base_idx]

# pick tail using simple tail proxy
def pick_tail_guard(X, idx, r, q=0.95):
    """ Take r members in the top-q% by Euclidean norm (simple tail proxy). """
    if r <= 0:
        return []
    norms = np.sqrt((X[idx]**2).sum(1))
    thr = np.quantile(norms, q)
    tail_idx = [idx[i] for i in np.where(norms >= thr)[0]]
    return tail_idx[:r]

# 3.1 Comparison between kmedoids, Neural Neighbour, and Hybrid clustering
def weigthed_kmedoids_plus_NN_clustering(halton):
    
    print("[INFO 031]: Hybrid kmedoids + NN refinement clustering")

    # convert stored lists back to 2D arrays for numerical operations
    halton_std_pts = np.asarray(list(halton['halton_std_pts']))
    halton_pts = np.asarray(list(halton['halton_pts']))

    # Uniform weights for anchor discovery to avoid over-fitting mass (just for medoids)
    sample_wgts = np.full(halton_std_pts.shape[0], 1.0 / halton_std_pts.shape[0])

    # --- clustering by kmedoids (initial)
    medoid_indices, _, _, _, medoid_dmin_pure = weighted_kmedoids(halton_std_pts, sample_wgts, k=K_INIT, seed=SEED)
    print(f"[INFO 031]: Initial medoid set: {K_INIT} reps, max coverage radius = {medoid_dmin_pure.max():.3f}")

    # --- NN refinement starts here

    # refine until threshold
    refined_indices, dmin_hybrid, max_dists = refine_until_threshold(
        halton_std_pts, medoid_indices, threshold=TARGET_THRESHOLD)
    print(f"[INFO 031]: Hybrid refinement produced {len(refined_indices)} representatives; max radius = {dmin_hybrid.max():.3f}")
  
    # find elbow
    elbow_idx, elbow_val = find_elbow(max_dists)
    print(f"[INFO 031]: Elbow suggests radius ~ {elbow_val:.3f} at refinement step {elbow_idx} (total reps ~ {K_INIT + elbow_idx})")

    # kneighbor fit after refinement
    scenario_pts = halton_std_pts[refined_indices]
    nn_model = NearestNeighbors(n_neighbors=1)
    nn_model.fit(scenario_pts)

    # compute probalility weights to be assign to each cluster center
    scenarion_prb_wgts = np.zeros(scenario_pts.shape[0], dtype=float)

    rng = np.random.default_rng(123)
    eval_pts = rng.normal(size=(NEVAL_PTS, NDIMS))
    labels = nn_model.kneighbors(eval_pts, return_distance=False)[0].ravel()
    counts = np.bincount(labels, minlength=scenario_pts.shape[0]).astype(float)
    scenario_prb_wgts = counts / counts.sum()

    return scenario_pts, scenario_prb_wgts, refined_indices, max_dists, medoid_indices


# 3.2. plot refinement process
def plot_refinement_process(max_dists):
    fig, ax = plt.subplots(figsize=(8,5))
    ax.plot(max_dists, marker='o')
    ax.axhline(TARGET_THRESHOLD, color='red', 
               linestyle='--', label='Target threshold')
    x_text = max(len(max_dists) - 1, 0) * 0.02
    ax.text(x_text, TARGET_THRESHOLD + 0.05, 'Target threshold',
            color='red', fontsize=10, ha='left', va='bottom')
    ax.set_xlabel('Refinement step')
    ax.set_ylabel('Max coverage radius')
    ax.set_title('Hybrid refinement convergence')
    ax.legend()
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)
    # ax.set_xlim(0, max(len(max_dists), 0))
    # ax.set_ylim(0.0, np.max(max_dists))


    info = "[INFO 032]: > Save figure of refinement process"
    savefigp(info, "032_refinement_process.png")

# 3.3. plot cluster comparison
def plot_cluster_comparison(halton, scenario_pts, refined_indices, medoid_indices):
    """scatter plot of samples: Halton, kmedoids, NN refinement and finally Hybrid representative"""     

    halton_pts = np.asarray(list(halton['halton_pts']))
    
    fig, ax = plt.subplots(figsize=(6, 6))
    x0, y0 = 6., 6.
    theta = np.linspace(0, 2*np.pi, 200)
    ct, st = np.cos(theta), np.sin(theta)
    scatter_size = 50
    for k in range(int(x0) + 4):
        ax.plot(k * ct, k * st, '-k', linewidth=0.4)
    ax.scatter(halton_pts[:, 0], halton_pts[:, 1], c='lightgray', s=scatter_size, 
                alpha=0.5, label='Scaled Halton pts')
    ax.scatter(halton_pts[medoid_indices, 0], halton_pts[medoid_indices, 1], facecolors='none', 
                edgecolors='tab:red', s=scatter_size, linewidths=1.2, label='Initial medoids')
    extra_indices = np.setdiff1d(refined_indices, medoid_indices)
    if extra_indices.size:
        ax.scatter(halton_pts[extra_indices, 0], halton_pts[extra_indices, 1], 
                    c='tab:blue', s=50, alpha=0.3, label='NN refinement')
    ax.scatter(scenario_pts[:, 0], scenario_pts[:, 1], c='tab:green', s=scatter_size, alpha=0.5, label='Hybrid reps')
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)
    ax.set_xlim((-x0 - 0.1, x0 + 0.1))
    ax.set_ylim((-y0 - 0.1, y0 + 0.1))
    ax.set_xlabel('$z_1$')
    ax.set_ylabel('$z_2$')
    leg = ax.legend(loc='upper right', fontsize=8, frameon=True)
    # ensure legend background is white and has a visible edge
    leg.get_frame().set_facecolor('white')
    leg.get_frame().set_edgecolor('black')

    info = "[INFO 033]: > Save figure for cluster comparisons"
    savefigp(info, "033_cluster_comparison.png")


# 3.4. Apply NN clustering
def NN_assignment(halton, medoid_idx):
    """Cell 7 — 1-NN assignment to anchors
    We assign every scenario to the nearest medoid; this is used for optional expansion and diagnostics.
    """

    print("[INFO 034]: Assigning scenarios to nearest medoid anchors via 1-NN")

    # Distance of each point to each medoid (uses your pairwise_dist)
    halton_std_wgts = halton['halton_std_wgts']
    # convert back to 2D arrays if the DataFrame stores per-sample vectors as lists
    halton_pts = np.asarray(list(halton['halton_pts']))
    D_m = pairwise_dist(halton_pts, halton_pts[medoid_idx])         # (n x K)
    assign_medoid_pos = D_m.argmin(axis=1)                          # 0..K-1
    assign_medoid_idx = medoid_idx[assign_medoid_pos]               # actual indices of medoids

    # Cluster membership lists (medoid -> members)
    clusters = {m: [] for m in medoid_idx}
    for i, m in enumerate(assign_medoid_idx):
        clusters[m].append(i)

    # Quick check: cluster masses by probability
    cluster_mass = {m: float(halton_std_wgts[clusters[m]].sum()) for m in medoid_idx}
    vals = list(cluster_mass.values())
    if len(vals) == 0:
        print("[WARN] cluster_mass is empty")
    else:
        vmin = float(np.min(vals))
        vmax = float(np.max(vals))
        vsum = float(np.sum(vals))
        print(f"[INFO 034]: Cluster mass stats: min={vmin:.4f} max={vmax:.4f} sum={vsum:.4f}")

    return clusters, assign_medoid_idx


# 3.5. cluster expansion
def cluster_expansion(halton, medoid_idx, clusters, expand_mode, r, q):
    """Cell 9 — Optional per-cluster expansion
    Pick extra members inside each cluster to improve shape/tails before final Voronoi reweighting. Set EXPAND_MODE="none" to skip.
    Expansion selction (optional)"""

    halton_pts = np.asarray(list(halton['halton_pts']))
    selected = set(medoid_idx.tolist())

    for m in medoid_idx:
        idx = clusters[m]
        if expand_mode == "rnn":
            extra = pick_rnn(halton_pts, idx, m, r=R_PER_CLUSTER)
        elif expand_mode == "diverse":
            extra = pick_diverse_farthest(halton_pts, idx, m, r=R_PER_CLUSTER)
        elif expand_mode == "tail":
            extra = pick_tail_guard(halton_pts, idx, r=R_PER_CLUSTER, q=TAIL_QUANTILE)
        else:
            extra = []
        selected.update(extra)

    selected = np.array(sorted(selected), dtype=int)
    print(f"[INFO 035]: Selected total (anchors + extras): {len(selected)}")

    return selected


# 3.6. final reweighting and save
def final_reweighting(halton, selected):
    """ Final reweighting via Voronoi by selected representatives. """
    halton_std_wgts = halton['halton_std_wgts']
    # ensure we have a 2D array for geometric ops
    halton_pts = np.asarray(list(halton['halton_pts']))
    # Reassign globally to nearest *selected* member
    D_sel = pairwise_dist(halton_pts, halton_pts[selected])   # (n x |S|)
    owner_pos = D_sel.argmin(axis=1)                  # nearest selected (position)
    final_weights = np.bincount(owner_pos, weights=halton_std_wgts, minlength=len(selected))
    final_weights = final_weights / final_weights.sum()

    # Representatives to save
    representative_pts  = halton_pts[selected]      # shape (K' x d)
    representative_prb_wgts = final_weights         # shape (K',)
    assert np.isclose(representative_prb_wgts.sum(), 1.0), "Weights must sum to 1."

    print(f"[INFO 036]: Saving {representative_pts.shape[0]} representatives.")
    info = "[INFO 036]: + save scenarios for HYBRID-01"
    save_scenario(info, "representative_pts.txt", representative_pts)
    save_scenario(info, "representative_prb_wgts.txt", representative_prb_wgts)

    return representative_pts, representative_prb_wgts, D_sel

# 3.7. Tiny sanity checks
def quick_diagnostics(selected, medoid_idx, D_sel):
    """
    Check that reduced set preserves mass at sites you care about.
    At minimum, compare full vs reduced means on your features; 
    better: compare exceedance curves at fgmax sites (if available in this notebook).    
    """           
    # mass per original medoid, post-selection
    sel_set = set(selected.tolist())
    kept_medoids = [m for m in medoid_idx if m in sel_set]
    dropped_medoids = [m for m in medoid_idx if m not in sel_set]
    print(f"[INFO 037]: Kept medoids: {len(kept_medoids)}, Dropped by design: {len(dropped_medoids)}")

    # eature coverage (coarse): mean & 95th-percentile distance to nearest selected
    dmin = D_sel.min(axis=1)
    print(f"[INFO 037]: mean NN-dist: {dmin.mean():.4f} | p95: {np.quantile(dmin,0.95):.4f}")


# (4) Visualization of the selected representatives ------------------------------------------

# lightweight PCA → 2D (NumPy only)
def pca_2d(X, weights=None):
    X = np.asarray(X, float)
    if weights is None:
        mu = X.mean(axis=0)
    else:
        w = np.asarray(weights, float)
        w = w / w.sum()
        mu = (w[:,None] * X).sum(axis=0)
    Xc = X - mu
    # economy SVD of covariance via direct SVD of centered data
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
    # 2D projection
    comps = Vt[:2].T                 # d x 2
    Z = Xc @ comps                   # n x 2
    return Z, comps, mu

from matplotlib.patches import Circle # type: ignore

# 4.1. Another plot comparison
def plot_comparison(halton):
    """
    Inputs needed from previous cells:
    - halton_pts         : (n,d) features
    - halton_std_wgts    : (n,) probabilities
    - medoid_idx         : (K,) anchor indices (initial medoids)
    - selected           : (K' ,) final selected indices (anchors + extras)
    - pca_2d(...)        : helper defined earlier
    """
    
    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = halton['halton_std_wgts']

    # --- PCA to 2D ---
    Z, comps, mu = pca_2d(halton_pts, weights=halton_std_wgts)

    # Masks
    sel_mask  = np.zeros(halton_pts.shape[0], dtype=bool)
    sel_mask[selected] = True
    med_mask  = np.zeros(halton_pts.shape[0], dtype=bool)
    med_mask[medoid_idx] = True
    extra_mask = sel_mask & (~med_mask)   # NN refinement (extras beyond anchors)

    # --- Figure setup ---
    fig, ax = plt.subplots(figsize=(7,7))

    # Concentric rings (light grey), centered at origin
    r = np.sqrt((Z**2).sum(axis=1))
    R = float(np.ceil(r.max()))
    for rr in np.arange(1.0, R+0.1, 1.0):
        ax.add_patch(Circle((0.0, 0.0), rr, fill=False, lw=0.8, alpha=0.4, color='0.7'))

    # Background cloud (light grey)
    ax.scatter(Z[:,0], Z[:,1], s=18, color='0.75', alpha=1.0, label="Scaled Halton", zorder=1)

    # Initial medoids (red hollow circles, on top of background)
    ax.scatter(Z[med_mask,0], Z[med_mask,1],
            s=60, facecolors='none', edgecolors='red', linewidths=0.5,
            label="Initial medoids", zorder=3)

    # NN refinement extras (blue filled)
    ax.scatter(Z[extra_mask,0], Z[extra_mask,1],
            s=40, alpha=0.5, label="NN refinement", zorder=4)

    # Hybrid reps = all selected (green filled)
    ax.scatter(Z[sel_mask,0], Z[sel_mask,1],
            s=40, color= 'green', alpha=0.75, label="Hybrid reps", zorder=2)

    # Axes, limits, legend
    ax.set_aspect('equal', adjustable='box')
    ax.set_xlim(-R, R); ax.set_ylim(-R, R)
    ax.legend(loc="upper right", framealpha=0.9)
    ax.grid(False)
    # Move axes to center (spines at zero)
    ax.spines['left'].set_position('zero')
    ax.spines['bottom'].set_position('zero')
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')
    ax.set_xlabel(r"$z_1$")
    ax.set_ylabel(r"$z_2$", rotation=0)
    ax.xaxis.set_label_coords(0.96, 0.53)
    ax.yaxis.set_label_coords(0.53, 0.97)

    # Remove "0" from tick labels (but keep the ticks at zero)
    xticks = ax.get_xticks()
    yticks = ax.get_yticks()
    ax.set_xticks(xticks)
    ax.set_yticks(yticks)
    ax.set_xticklabels([f"{x:g}" if x != 0 else "" for x in xticks])
    ax.set_yticklabels([f"{y:g}" if y != 0 else "" for y in yticks])
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)

    info = "[INFO 041]: > Save figure for comparison cluster"
    savefigp(info, "041_PCA_2D_comparison_cluster.png")


# 4.2. Approximation error: anchors-only vs hybrid
def plot_approximation_error(halton, medoid_idx):
    # Build anchors-only set and distances
    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = halton['halton_std_wgts']

    D_anchor = pairwise_dist(halton_pts, halton_pts[medoid_idx])   # (n x K)
    dmin_anchor = D_anchor.min(axis=1)

    # Hybrid distances (already computed): D_sel and dmin_hybrid
    dmin_hybrid = pairwise_dist(halton_pts, halton_pts[selected]).min(axis=1)

    # Weighted summary
    def wstats(x, w):
        w = w / w.sum()
        xs = np.sort(x); ws = w[np.argsort(x)]
        mean = (w * x).sum()
        cdf = np.cumsum(ws)
        p95 = xs[np.searchsorted(cdf, 0.95)]
        return mean, p95

    ma, qa = wstats(dmin_anchor, halton_std_wgts)
    mh, qh = wstats(dmin_hybrid, halton_std_wgts)

    print("[INFO 052]: Approximation error comparison") 
    print(f"[Approx error | weighted NN distance]")
    print(f"  Anchors: mean={ma:.4f}, p95={qa:.4f}")
    print(f"  Hybrid : mean={mh:.4f}, p95={qh:.4f}")

    # Histogram overlay
    fig, ax = plt.subplots(figsize=(7,5))
    bins = np.linspace(0, max(dmin_anchor.max(), dmin_hybrid.max()), 40)
    ax.hist(dmin_anchor, bins=bins, alpha=0.5, label="Anchors-only", weights=halton_std_wgts)
    ax.hist(dmin_hybrid, bins=bins, alpha=0.5, label="Hybrid", weights=halton_std_wgts)
    ax.set_xlabel("Distance to nearest selected"); plt.ylabel("Weighted count")
    ax.set_title("Hybrid reduces approximation error (mean & tail)"); plt.legend(); plt.tight_layout()
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)

    info = "[INFO 042]: > Save histogram of approximation error"
    savefigp(info, "042_Histogram_approximation_error.png")

# 4.3. Quantitative summary of approximation error reduction
def summary_approximation_error(halton, medoid_idx):
    def weighted_quantile(x, w, qs=(0.95,0.99)):
        """Compute weighted quantiles."""
        x = np.asarray(x); w = np.asarray(w)
        sorter = np.argsort(x)
        x, w = x[sorter], w[sorter]
        cdf = np.cumsum(w) / np.sum(w)
        return np.interp(qs, cdf, x)

    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = halton['halton_std_wgts']

    # Anchors-only distances (computed earlier)
    D_anchor = pairwise_dist(halton_pts, halton_pts[medoid_idx])
    dmin_anchor = D_anchor.min(axis=1)

    # Hybrid distances
    D_hybrid = pairwise_dist(halton_pts, halton_pts[selected])
    dmin_hybrid = D_hybrid.min(axis=1)

    # Weighted stats
    mean_anchor = np.average(dmin_anchor, weights=halton_std_wgts)
    mean_hybrid = np.average(dmin_hybrid, weights=halton_std_wgts)
    q95_anchor, q99_anchor = weighted_quantile(dmin_anchor, halton_std_wgts, (0.95,0.99))
    q95_hybrid, q99_hybrid = weighted_quantile(dmin_hybrid, halton_std_wgts, (0.95,0.99))

    # Improvement percentages
    improve_mean = 100 * (mean_anchor - mean_hybrid) / mean_anchor
    improve_95 = 100 * (q95_anchor - q95_hybrid) / q95_anchor
    improve_99 = 100 * (q99_anchor - q99_hybrid) / q99_anchor

    summary_df = pd.DataFrame({
        "Metric": ["Weighted mean", "95th percentile", "99th percentile"],
        "Anchors-only": [mean_anchor, q95_anchor, q99_anchor],
        "Hybrid": [mean_hybrid, q95_hybrid, q99_hybrid],
        "Improvement (%)": [improve_mean, improve_95, improve_99]
    })

    print("[INFO 043]: Quantitative summary of approximation error reduction ")
    fmt_summary = {
        "Anchors-only": "{:.3f}".format,
        "Hybrid": "{:.3f}".format,
        "Improvement (%)": "{:+.1f}".format,
    }
    print(summary_df.to_string(index=False, formatters=fmt_summary))
        
    return summary_df


# 4.4. plot cluster probability mass
def plot_cluster_probability_mass(halton, medoid_idx):
    """ Hybrid selection reduces both the average and tail distances between full scenarios 
    and their nearest representative compared with K-Medoids only, 
    confirming stronger coverage of the scenario space.
    """  

    halton_std_wgts = halton['halton_std_wgts']

    # Probability mass per anchor cluster (before adding extras)
    cluster_mass = {m: float(halton_std_wgts[np.array(clusters[m])].sum()) for m in medoid_idx}
    pairs = sorted(cluster_mass.items(), key=lambda kv: kv[1], reverse=True)

    top = min(30, len(pairs))
    labels = [str(k) for k,_ in pairs[:top]]
    vals   = [v for _,v in pairs[:top]]

    fig, ax = plt.subplots(figsize=(8,4))
    ax.bar(range(top), vals)
    ax.set_xticks(range(top), labels, rotation=90)
    ax.set_ylabel("Probability mass"); plt.xlabel("Medoid index (top 30)")
    ax.set_title("Some anchor clusters carry much more mass → good spots to add extras")
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)

    info = "[INFO 044]: > Save histogram of probability mass"
    savefigp(info, "044_Histogram_probability_mass_.png")

# 4.5. compute tail coverages
def compute_tail_coverages(D_anchor, halton, medoid_idx, selected, wgts_sel):
    """ECDF of full vs hybrid reconstruction:
    For hybrid, the "reconstructed" distribution is represented by selected points with wgts_sel.
    Compare the ECDFs; tails (e.g., 95%+) should match better than anchors-only.
    Tail proxy: Euclidean norm in feature space (works for any d)"""

    # EDF helper
    def ecdf(x, w):
        o = np.argsort(x)
        xs = x[o]; ws = w[o] / w.sum()
        F = np.cumsum(ws)
        return xs, F

    dmin_anchor = D_anchor.min(axis=1)
    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = halton['halton_std_wgts']
    norms = np.sqrt((halton_pts**2).sum(1))
    xs_full, F_full = ecdf(norms, halton_std_wgts)
    xs_sel  = np.sqrt((halton_pts[selected]**2).sum(1))
    xs_h, F_h = ecdf(xs_sel, wgts_sel)

    owner_anchor = dmin_anchor.argmin()  # not used; build properly below
    owner_pos_anchor = D_anchor.argmin(axis=1)
    w_anchor = np.bincount(owner_pos_anchor, weights=halton_std_wgts, minlength=len(medoid_idx))
    w_anchor = w_anchor / w_anchor.sum()
    xs_anchor = np.sqrt((halton_pts[medoid_idx]**2).sum(1))
    xa, Fa = ecdf(xs_anchor, w_anchor)

    fig, ax = plt.subplots(figsize=(7,5))
    ax.plot(xs_full, F_full,'.', label="Full ensemble")
    ax.plot(xs_anchor, Fa,'*', label="Anchors-only (Voronoi)")
    ax.plot(xs_sel, F_h,'+', label="Hybrid (Voronoi)")
    ax.set_xlabel("Feature norm (tail proxy)"); plt.ylabel("ECDF")
    ax.set_title("Tail coverage: Hybrid should track full ECDF better than anchors-only")
    ax.legend(loc='lower right'); plt.tight_layout()
    ax.set_xlim(5, NDIMS)
    ax.xaxis.set_ticks_position('both')
    ax.yaxis.set_ticks_position('both')
    ax.tick_params(axis='both', which='both', direction='in', top=True, right=True)

    info = "[INFO 045]: > Save figure of tail coverages"
    savefigp(info, "045_tail_coverage.png")

    # Return a plain dict of arrays (lengths differ across entries)
    tail_coverage = {
        'owner_anchor'  : owner_anchor,
        'xs_full'       : xs_full,
        'F_full'        : F_full,
        'xs_sel'        : xs_sel,
        'xs_h'          : xs_h,
        'F_h'           : F_h,
        'wa_anchor'     : w_anchor,
        'xs_anchor'     : xs_anchor,
        'Fa'            : Fa,
        'xa'            : xa,
    }

    return tail_coverage

# 4.6. Compute tail error matrics
def compute_tail_error_metrics(halton, tail_coverage, wgts_sel):
    """Tail-error metrics (KS, Integrated ECDF error, Wasserstein-1), overall and tail-only"""

    halton_std_wgts = halton['halton_std_wgts']
    halton_pts = np.asarray(list(halton['halton_pts']))

    # ensure arrays are numpy arrays (tail_coverage may carry numpy or pandas types)
    xs_full = np.asarray(tail_coverage['xs_full'])
    F_full = np.asarray(tail_coverage['F_full'])
    xs_sel = np.asarray(tail_coverage['xs_sel'])
    F_h = np.asarray(tail_coverage['F_h'])
    xs_anchor = np.asarray(tail_coverage['xs_anchor'])
    w_anchor = np.asarray(tail_coverage['wa_anchor'])
    Fa  = np.asarray(tail_coverage['Fa'])
    
    # ---- Helpers: weighted ECDF & quantile function ----
    def wsort(x, w):
        o = np.argsort(x)
        return x[o], w[o]

    def ecdf_xy(x, w):
        x, w = np.asarray(x, float), np.asarray(w, float)
        x, w = wsort(x, w)
        F = np.cumsum(w) / w.sum()
        return x, F

    def qfun(p, x, F):
        # weighted quantile: inverse ECDF on [0,1]
        p = np.clip(np.asarray(p, float), 0.0, 1.0)
        return np.interp(p, F, x)

    def ks_distance(x_ref, F_ref, x_cmp, F_cmp):
        # Evaluate both ECDFs on the union grid
        grid = np.unique(np.concatenate([x_ref, x_cmp]))
        F_ref_g = np.interp(grid, x_ref, F_ref, left=0.0, right=1.0)
        F_cmp_g = np.interp(grid, x_cmp, F_cmp, left=0.0, right=1.0)
        return float(np.max(np.abs(F_ref_g - F_cmp_g)))

    def l1_ecdf_area(x_ref, F_ref, x_cmp, F_cmp):
        # ∫ |F_ref - F_cmp| dx over support (Riemann on union grid)
        grid = np.unique(np.concatenate([x_ref, x_cmp]))
        F_ref_g = np.interp(grid, x_ref, F_ref, left=0.0, right=1.0)
        F_cmp_g = np.interp(grid, x_cmp, F_cmp, left=0.0, right=1.0)
        diff = np.abs(F_ref_g - F_cmp_g)
        # trapezoid rule on x
        return float(np.trapezoid(diff, grid))

    def wasserstein1(x_ref, w_ref, x_cmp, w_cmp, ngrid=2000):
        # W1 = ∫_0^1 |Q_ref(p) - Q_cmp(p)| dp (1D EMD)
        xr, Fr = ecdf_xy(x_ref, w_ref)
        xc, Fc = ecdf_xy(x_cmp, w_cmp)
        p = np.linspace(0, 1, ngrid)
        qr = qfun(p, xr, Fr)
        qc = qfun(p, xc, Fc)
        return float(np.trapezoid(np.abs(qr - qc), p))

    # Build references
    x_ref, F_ref = xs_full, F_full
    norms = np.sqrt((halton_pts**2).sum(1))

    # Overall metrics
    metrics = []
    for name, x_cmp, F_cmp, w_cmp in [
        ("K-Medoids only", xs_anchor, Fa, w_anchor),
        ("Hybrid",         xs_sel,   F_h, wgts_sel),
    ]:
        ks   = ks_distance(x_ref, F_ref, x_cmp, F_cmp)
        l1   = l1_ecdf_area(x_ref, F_ref, x_cmp, F_cmp)
        w1   = wasserstein1(norms, halton_std_wgts, x_cmp, w_cmp)
        metrics.append((name, ks, l1, w1))

    # Tail-only metrics (e.g., top 10% of the full ensemble)
    tail_q = 0.90
    xq = qfun(tail_q, x_ref, F_ref)    # threshold x where F_full = 0.90
    mask_full   = norms >= xq
    # reduce to tail supports:
    def tail_from_ecdf(x, F, xmin):
        m = x >= xmin
        return x[m], (F[m] - F[m][0]) / (1.0 - F[m][0] + 1e-15)  # re-normalize 0→1

    x_ref_t, F_ref_t = tail_from_ecdf(x_ref, F_ref, xq)

    tail_metrics = []
    for name, x_cmp, F_cmp, w_cmp in [
        ("K-Medoids only", xs_anchor, Fa, w_anchor),
        ("Hybrid",         xs_sel,   F_h, wgts_sel),
    ]:
        x_cmp_t, F_cmp_t = tail_from_ecdf(x_cmp, F_cmp, xq)
        ks_t = ks_distance(x_ref_t, F_ref_t, x_cmp_t, F_cmp_t)
        l1_t = l1_ecdf_area(x_ref_t, F_ref_t, x_cmp_t, F_cmp_t)
        # W1 on tail using quantiles (re-normalized)
        p = np.linspace(0, 1, 1500)
        qr = qfun(p, x_ref_t, F_ref_t)
        qc = qfun(p, x_cmp_t, F_cmp_t)
        w1_t = float(np.trapezoid(np.abs(qr - qc), p))
        tail_metrics.append((name, ks_t, l1_t, w1_t))

    # Pretty tables
    df_overall = pd.DataFrame(metrics, columns=["Method","KS (sup |ΔF|)","∫|ΔF| dx","Wasserstein-1"])
    df_tail    = pd.DataFrame(tail_metrics, columns=["Method","Tail KS","Tail ∫|ΔF| dx","Tail W1"])

    print(f"[INFO 046]:Tail threshold set at the {int(tail_q*100)}th percentile of the FULL ensemble: x ≥ {xq:.3f}\n")
    fmt_overall = {
        "KS (sup |ΔF|)": "{:.4f}".format,
        "∫|ΔF| dx": "{:.4f}".format,
        "Wasserstein-1": "{:.4f}".format,
    }
    fmt_tail = {
        "Tail KS": "{:.4f}".format,
        "Tail ∫|ΔF| dx": "{:.4f}".format,
        "Tail W1": "{:.4f}".format,
    }
    print(df_overall.to_string(index=False, formatters=fmt_overall))
    print(df_tail.to_string(index=False, formatters=fmt_tail))

    return df_overall, df_tail, tail_q, xq, w_anchor


# 4.7. Improvement summary (Hybrid vs K-Medoids only)
def compute_improvement(df, label_ref="K-Medoids only", label_cmp="Hybrid"):
    """
    Compute percent improvement = 100*(ref - cmp)/ref for each numeric column.
    Positive => Hybrid smaller (better).
    """
    ref = df[df["Method"] == label_ref].iloc[0]
    cmp = df[df["Method"] == label_cmp].iloc[0]
    out = {}
    for col in df.columns:
        if col == "Method": 
            continue
        ref_val, cmp_val = ref[col], cmp[col]
        out[col] = 100 * (ref_val - cmp_val) / ref_val if ref_val != 0 else np.nan

    return pd.Series(out)

# 4.8. Summary printout
def improvement_summary(df_overall, df_tail):
    imp_overall = compute_improvement(df_overall)
    imp_tail    = compute_improvement(df_tail)
    imp_df = pd.DataFrame({
        "Metric": imp_overall.index,
        "Overall Improvement (%)": imp_overall.values,
        "Tail Improvement (%)": imp_tail.values
    }).round(1)

    print("[INFO 048]: Hybrid improvement relative to K-Medoids only (positive = better)\n")
    fmt_imp = {
        "Overall Improvement (%)": "{:+.1f}".format,
        "Tail Improvement (%)": "{:+.1f}".format,
    }
    print(imp_df.to_string(index=False, formatters=fmt_imp))

    # Optional concise text summary
    def make_summary_line(row):
        return f"{row['Metric']}: overall {row['Overall Improvement (%)']:+.1f} %, tail {row['Tail Improvement (%)']:+.1f} %"

    summary_lines = [make_summary_line(r) for _,r in imp_df.iterrows()]
    print("[INFO 048]: Concise improvement summary:")
    print("\n".join(summary_lines))


# (5) Visualization of improvements ---------------------------------------------------------------

# 5.1. Visualisation ofPer-feature moments/quantiles: full vs hybrid
def plot_per_feature_comparison(halton, selected, wgts_sel):

    def wmean(x, w): 
        w = w / w.sum()
        return (w[:,None]*x).sum(axis=0)

    def wq(x, w, q):
        x = np.asarray(x); w = np.asarray(w); w = w/w.sum()
        out = []
        for j in range(x.shape[1]):
            o = np.argsort(x[:,j])
            xs, ws = x[o,j], w[o]
            F = np.cumsum(ws)
            out.append(xs[np.searchsorted(F, q)])
        return np.array(out)

    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = np.asarray(halton['halton_std_wgts'])
    
    mu_full  = wmean(halton_pts, halton_std_wgts)
    mu_hyb   = wmean(halton_pts[selected], wgts_sel)
    q95_full = wq(halton_pts, halton_std_wgts, 0.95)
    q95_hyb  = wq(halton_pts[selected], wgts_sel, 0.95)

    idxs = np.arange(halton_pts.shape[1])

    fig, ax= plt.subplots(2,1, figsize=(9,4), sharex=True)
    ax[0].bar(idxs-0.2, mu_full,  width=0.4, label="Full")
    ax[0].bar(idxs+0.2, mu_hyb,   width=0.4, label="Hybrid")
    ax[0].set_title("Per-feature weighted mean"); plt.xlabel("Feature index"); plt.legend()
    ax[0].set_xlim(0, halton_pts.shape[1])


    ax[1].bar(idxs-0.2, q95_full, width=0.4, label="Full")
    ax[1].bar(idxs+0.2, q95_hyb,  width=0.4, label="Hybrid")
    ax[1].set_title("Per-feature 95th percentile"); plt.xlabel("Feature index"); plt.legend()

    info = "[INFO 051]: > Save figure of per feature comparison"
    savefigp(info, "051_Per_feature_comparison.png")

    return mu_full, mu_hyb, q95_full, q95_hyb

# 5.2. Quantitative summary of per-feature mean & tail preservation
def quantitative_per_feature_summary(mu_full, mu_hyb, q95_full, q95_hyb):
    # --- Compute absolute and relative differences per feature ---
    abs_err_mean = np.abs(mu_hyb - mu_full)
    rel_err_mean = smape_percent(mu_hyb, mu_full)     # symmetric % keeps values bounded

    abs_err_q95  = np.abs(q95_hyb - q95_full)
    rel_err_q95  = smape_percent(q95_hyb, q95_full)

    # --- Overall summary stats ---
    summary = {
        "Mean abs. error (mean)": abs_err_mean.mean(),
        "Mean rel. error % (mean)": rel_err_mean.mean(),
        "Mean abs. error (95th)": abs_err_q95.mean(),
        "Mean rel. error % (95th)": rel_err_q95.mean(),
        "Max rel. error % (mean)": rel_err_mean.max(),
        "Max rel. error % (95th)": rel_err_q95.max(),
    }

    summary_df = pd.DataFrame([summary]).T.rename(columns={0:"Value"})
    print("[INFO 052]: === Summary of feature-wise deviation between Full and Hybrid ===")
    print(summary_df.to_string(formatters={"Value": "{:.3f}".format}))

    # --- Detailed per-feature table (optional) ---
    df_feat = pd.DataFrame({
        "Feature": np.arange(len(mu_full)),
        "|Δ mean|": abs_err_mean,
        "Rel. error % (mean)": rel_err_mean,
        "|Δ 95th|": abs_err_q95,
        "Rel. error % (95th)": rel_err_q95
    })
    fmt_feat = {col: "{:.3f}".format for col in df_feat.columns if col != "Feature"}
    print(df_feat.to_string(index=False, formatters=fmt_feat))

    return summary_df, df_feat


# 5.3. Compare K-Medoids only and Hybrid performance per feature
def per_feature_comparison_kmed_vs_hyb(halton, medoid_idx, selected, wgts_sel):
    
    def wmean(x, w): 
        w = w / w.sum()
        return (w[:,None]*x).sum(axis=0)

    def wq(x, w, q):
        x = np.asarray(x); w = np.asarray(w); w = w/w.sum()
        out = []
        for j in range(x.shape[1]):
            o = np.argsort(x[:,j])
            xs, ws = x[o,j], w[o]
            F = np.cumsum(ws)
            out.append(xs[np.searchsorted(F, q)])
        return np.array(out)

    # Compute stats for full ensemble (convert stored lists/Series to numpy)
    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = np.asarray(halton['halton_std_wgts'])
    mu_full  = wmean(halton_pts, halton_std_wgts)
    q95_full = wq(halton_pts, halton_std_wgts, 0.95)

    # Compute stats for Hybrid representatives
    mu_hyb   = wmean(halton_pts[selected], wgts_sel)
    q95_hyb  = wq(halton_pts[selected], wgts_sel, 0.95)

    # Compute the same stats for K-Medoids-only representatives
    # (use your medoid_idx and w_anchor already defined earlier)
    mu_kmed  = wmean(halton_pts[medoid_idx], w_anchor)
    q95_kmed = wq(halton_pts[medoid_idx], w_anchor, 0.95)

    ensamble = pd.DataFrame({
        "mu_full": mu_full,
        "mu_hyb": mu_hyb,
        "mu_kmed": mu_kmed,
        "q95_full": q95_full,
        "q95_hyb": q95_hyb,
        "q95_kmed": q95_kmed,
    })

    # Absolute and relative errors versus full ensemble
    abs_err_mean_kmed = np.abs(mu_kmed - mu_full)
    rel_err_mean_kmed = smape_percent(mu_kmed, mu_full)
    abs_err_q95_kmed  = np.abs(q95_kmed - q95_full)
    rel_err_q95_kmed  = smape_percent(q95_kmed, q95_full)

    abs_err_mean_hyb = np.abs(mu_hyb - mu_full)
    rel_err_mean_hyb = smape_percent(mu_hyb, mu_full)
    abs_err_q95_hyb  = np.abs(q95_hyb - q95_full)
    rel_err_q95_hyb  = smape_percent(q95_hyb, q95_full)

    # Mean improvements (positive → Hybrid smaller error)
    improve_mean_rel = 100 * (rel_err_mean_kmed.mean() - rel_err_mean_hyb.mean()) / rel_err_mean_kmed.mean()
    improve_q95_rel  = 100 * (rel_err_q95_kmed.mean()  - rel_err_q95_hyb.mean())  / rel_err_q95_kmed.mean()

    error_metrics = pd.DataFrame({
        'abs_err_mean_kmed' : abs_err_mean_kmed,
        'rel_err_mean_kmed' : rel_err_mean_kmed,
        'abs_err_q95_kmed'  : abs_err_q95_kmed,
        'rel_err_q95_kmed'  : rel_err_q95_kmed,
        'abs_err_mean_hyb'  : abs_err_mean_hyb,
        'rel_err_mean_hyb'  : rel_err_mean_hyb,
        'abs_err_q95_hyb'   : abs_err_q95_hyb,
        'rel_err_q95_hyb'   : rel_err_q95_hyb,
        'improve_mean_rel' : improve_mean_rel,
        'improve_q95_rel'  : improve_q95_rel,
    })

    summary_comp = {
        "Mean rel. error % (mean)": [rel_err_mean_kmed.mean(), rel_err_mean_hyb.mean(), improve_mean_rel],
        "Mean rel. error % (95th)": [rel_err_q95_kmed.mean(), rel_err_q95_hyb.mean(), improve_q95_rel],
        "Max rel. error % (mean)":  [rel_err_mean_kmed.max(), rel_err_mean_hyb.max(), np.nan],
        "Max rel. error % (95th)":  [rel_err_q95_kmed.max(), rel_err_q95_hyb.max(), np.nan],
    }
    df_comp = pd.DataFrame(summary_comp, index=["K-Medoids only","Hybrid","Improvement (%)"])
    print("[INFO 053]: === Comparison: K-Medoids only vs Hybrid (per-feature mean & tail errors) ===")
    print(df_comp.to_string(float_format="{:.2f}".format))

    return ensamble,error_metrics, summary_comp, df_comp 


# 5.4. Per-feature comparison: K-Medoids only vs Hybrid
def quantitative_per_feature_comparison_kmed_vs_hyb(ensamble):
    # Another quantitative summary cell you can paste right after your Viz-7 
    # (per-feature means & percentiles) to compare K-Medoids only vs Hybrid performance.


    mu_full = ensamble['mu_full']
    mu_hyb =  ensamble['mu_hyb']
    mu_kmed = ensamble['mu_kmed']
    q95_full = ensamble['q95_full']
    q95_hyb = ensamble['q95_hyb'] 
    q95_kmed = ensamble['q95_kmed']

    # Already available: mu_full, mu_hyb, mu_kmed, q95_full, q95_hyb, q95_kmed, fitpts
    # Compute robust scale (IQR) for normalization
    iqr_full = np.subtract(*np.percentile(halton_pts, [75, 25], axis=0))
    iqr_full = np.where(iqr_full < EPS, EPS, iqr_full)  # avoid divide-by-zero

    # --- 1) SMAPE for means (stable near zero) ---
    smape_mean_kmed = smape_percent(mu_kmed, mu_full)
    smape_mean_hyb  = smape_percent(mu_hyb,  mu_full)

    # --- 2) IQR-normalized absolute errors (dimensionless, robust) ---
    iqrerr_mean_kmed  = np.abs(mu_kmed - mu_full)/(iqr_full + EPS)
    iqrerr_mean_hyb   = np.abs(mu_hyb  - mu_full)/(iqr_full + EPS)
    iqrerr_q95_kmed   = np.abs(q95_kmed - q95_full)/(iqr_full + EPS)
    iqrerr_q95_hyb    = np.abs(q95_hyb  - q95_full)/(iqr_full + EPS)

    iqr_error = pd.DataFrame({
        'iqrerr_mean_kmed' : iqrerr_mean_kmed,  
        'iqrerr_mean_hyb'  : iqrerr_mean_hyb,
        'iqrerr_q95_kmed'  : iqrerr_q95_kmed,
        'iqrerr_q95_hyb'   : iqrerr_q95_hyb,
    })

    # --- Summaries for each metric ---
    def summarize(arr):
        return pd.Series({
            "Mean": np.mean(arr),
            "Median": np.median(arr),
            "P90": np.percentile(arr, 90),
            "Max": np.max(arr)
        })

    tbl = pd.DataFrame({
        "SMAPE mean (K-Medoids)": summarize(smape_mean_kmed),
        "SMAPE mean (Hybrid)":    summarize(smape_mean_hyb),
        "IQR-norm |Δmean| (K-M)": summarize(iqrerr_mean_kmed),
        "IQR-norm |Δmean| (Hybr)":summarize(iqrerr_mean_hyb),
        "IQR-norm |Δ95th| (K-M)": summarize(iqrerr_q95_kmed),
        "IQR-norm |Δ95th| (Hybr)":summarize(iqrerr_q95_hyb),
    }).T

    print("[INFO 054]: === IQR-normalized per-feature deviation: K-Medoids vs Hybrid ===")
    print(tbl.to_string(float_format="{:.3f}".format))

    # --- Overall improvements (positive = Hybrid better) ---
    imp_smape_mean = 100*(np.mean(smape_mean_kmed) - np.mean(smape_mean_hyb))/(np.mean(smape_mean_kmed)+EPS)
    imp_iqr_mean   = 100*(np.mean(iqrerr_mean_kmed) - np.mean(iqrerr_mean_hyb))/(np.mean(iqrerr_mean_kmed)+EPS)
    imp_iqr_q95    = 100*(np.mean(iqrerr_q95_kmed)  - np.mean(iqrerr_q95_hyb))/(np.mean(iqrerr_q95_kmed)+EPS)

    # Max improvements (optional)
    max_imp_mean = 100*(np.max(iqrerr_mean_kmed) - np.max(iqrerr_mean_hyb))/(np.max(iqrerr_mean_kmed)+EPS)
    max_imp_q95  = 100*(np.max(iqrerr_q95_kmed)  - np.max(iqrerr_q95_hyb))/(np.max(iqrerr_q95_kmed)+EPS)

    print(f"Improvement (Hybrid vs K-Medoids):")
    print(f"  SMAPE(mean): {imp_smape_mean:+.1f}%")
    print(f"  IQR-norm |Δmean|: {imp_iqr_mean:+.1f}%")
    print(f"  IQR-norm |Δ95th|: {imp_iqr_q95:+.1f}%")
    print(f"  Max improvement (IQR-norm): mean {max_imp_mean:+.1f}%, 95th {max_imp_q95:+.1f}%")

    improvement = pd.DataFrame({
        'imp_smape_mean'    : [imp_smape_mean],
        'imp_iqr_mean'      : [imp_iqr_mean],
        'imp_iqr_q95'       : [imp_iqr_q95],
        'max_imp_mean'      : [max_imp_mean],
        'max_imp_q95'       : [max_imp_q95],
    })

    return iqr_error, tbl, improvement

# 5.5. Tail-guard selection

# Full-ensemble 95th per feature (helper function)
def wq_vec(X, w, q=0.95):
    X = np.asarray(X); w = np.asarray(w); w = w / w.sum()
    out = []
    for j in range(X.shape[1]):
        o = np.argsort(X[:,j])
        xs, ws = X[o, j], w[o]
        F = np.cumsum(ws)
        out.append(xs[np.searchsorted(F, q)])
    return np.array(out)

def tail_guard_selection(halton, clusters, selected, wgts_sel):
    """Assumes you already have: halton_pts, halton_std_wgts, medoid_idx, clusters (dict medoid -> member indices),
    selected (np.ndarray of selected indices), wgts_sel (current weights), and wq(), iqr_full, etc."""

    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = np.asarray(halton['halton_std_wgts'])

    # --- Recompute robust scale & tail stats (in case you changed selection before) ---
    iqr_full = np.subtract(*np.percentile(halton_pts, [75, 25], axis=0))
    iqr_full = np.where(iqr_full < EPS, EPS, iqr_full)

    # 95th-percentile per feature
    q95_full  = wq_vec(halton_pts, halton_std_wgts, 0.95)
    q95_hyb   = wq_vec(halton_pts[selected], wgts_sel, 0.95)

    # IQR-normalized 95th errors (per feature)
    iqrerr_q95_hyb = np.abs(q95_hyb - q95_full) / (iqr_full + EPS)
    j_star = int(np.argmax(iqrerr_q95_hyb))     # worst offending feature

    print(f"[INFO 055]: TAIL-GUARD Worst tail feature index = {j_star} "
        f"(IQR-norm |Δ95th| = {iqrerr_q95_hyb[j_star]:.3f})")

    # --- For each cluster, pick the most extreme (high) unselected member in feature j_star ---
    sel_set = set(selected.tolist())
    extras_guard = []

    for m, idx in clusters.items():
        if not idx:
            continue
        idx_arr = np.array(idx, dtype=int)
        # exclude already selected
        pool = [i for i in idx_arr if i not in sel_set]
        if not pool:
            continue
        pool = np.array(pool, dtype=int)
        # rank by feature value in j_star (largest = most extreme tail)
        order = np.argsort(halton_pts[pool, j_star])[::-1]
        pick = pool[order[0]]
        extras_guard.append(pick)

    extras_guard = np.array(sorted(set(extras_guard)), dtype=int)
    print(f"[TAIL-GUARD] Proposed extras = {len(extras_guard)} per-cluster guard points.")

    tail_guard = {
        'extras_guard': extras_guard,
        'iqrerr_q95_hyb': iqrerr_q95_hyb,
        'q95_full': q95_full,
        'q95_hyb': q95_hyb,
        'iqr_full': iqr_full,
        'j_star': j_star,
    }

    return tail_guard


# 5.6. Apply tail-guard selection
def apply_tail_guard(halton, selected, tailguard):
    
    
    # convert stored list-of-vectors back into numerical arrays
    halton_pts = np.asarray(list(halton['halton_pts']))
    halton_std_wgts = np.asarray(halton['halton_std_wgts'])
    iqrerr_q95_hyb = tailguard['iqrerr_q95_hyb']
    extras_guard = tailguard['extras_guard']
    q95_full = tailguard['q95_full']
    iqr_full = tailguard['iqr_full']
    j_star = int(tailguard['j_star'])


    # Apply guard picks
    selected_guard = np.unique(np.concatenate([selected, extras_guard])).astype(int)

    # Recompute Voronoi weights with the augmented set
    from numpy.linalg import norm

    D_sel_guard = pairwise_dist(halton_pts, halton_pts[selected_guard])
    owner_pos_guard = D_sel_guard.argmin(axis=1)
    wgts_sel_guard = np.bincount(owner_pos_guard, weights=halton_std_wgts, minlength=len(selected_guard))
    wgts_sel_guard = wgts_sel_guard / wgts_sel_guard.sum()

    # Recompute 95th-per-feature and IQR-norm tail errors AFTER guard
    q95_hyb_guard = wq_vec(halton_pts[selected_guard], wgts_sel_guard, 0.95)
    iqrerr_q95_guard = np.abs(q95_hyb_guard - q95_full) / (iqr_full + EPS)

    # Summaries
    mean_before = float(np.mean(iqrerr_q95_hyb))
    mean_after  = float(np.mean(iqrerr_q95_guard))
    max_before  = float(np.max(iqrerr_q95_hyb))
    max_after   = float(np.max(iqrerr_q95_guard))
    imp_mean = 100*(mean_before - mean_after)/(mean_before + EPS)
    imp_max  = 100*(max_before  - max_after )/(max_before  + EPS)

    print("[INFO 056]: TAIL-GUARD -> IQR-norm |Δ95th| summary (all features):")
    print(f"  Mean: before {mean_before:.3f} → after {mean_after:.3f}  (improvement {imp_mean:+.1f}%)")
    print(f"  Max : before {max_before:.3f} → after {max_after:.3f}  (improvement {imp_max:+.1f}%)")

    # Also report for the worst feature j_star
    before_star = float(iqrerr_q95_hyb[j_star])
    after_star  = float(iqrerr_q95_guard[j_star])
    imp_star = 100*(before_star - after_star)/(before_star + EPS)
    print(f"[INFO 056]: TAIL-GUARD-> Feature j*={j_star}: before {before_star:.3f} → after {after_star:.3f} "
        f"(improvement {imp_star:+.1f}%)")

    # Final save of updated scenario with tail-guard
    selected   = selected_guard
    wgts_sel   = wgts_sel_guard
    final_pts = halton_pts[selected]
    final_wgts = wgts_sel

    info = f"[INFO 056]: + Saving {final_pts.shape[0]} representatives."
    save_scenario(info, "final_pts.txt", final_pts)
    save_scenario(info, "final_prb_wgts.txt", final_wgts)
    
    return final_pts, final_wgts

# 5.7. Plot per-feature IQR-normalized error comparison
def plot_per_feature_iqr_comparison(iqr_error):
        
    # setup feature for plot
    n_features = 20
    features = np.arange(1, n_features + 1)
    width = 0.35

    # Extract IQR-normalized errors
    iqrerr_mean_kmed = iqr_error['iqrerr_mean_kmed']
    iqrerr_mean_hyb = iqr_error['iqrerr_mean_hyb']
    iqrerr_q95_kmed = iqr_error['iqrerr_q95_kmed']
    iqrerr_q95_hyb = iqr_error['iqrerr_q95_hyb']
    
    # Average improvement annotations
    mean_imp  = 100 * (np.mean(iqrerr_mean_kmed[:n_features]) - np.mean(iqrerr_mean_hyb[:n_features])) / np.mean(iqrerr_mean_kmed[:n_features])
    tail_imp  = 100 * (np.mean(iqrerr_q95_kmed[:n_features])  - np.mean(iqrerr_q95_hyb[:n_features]))  / np.mean(iqrerr_q95_kmed[:n_features])

    fig, ax = plt.subplots(2, 1, figsize=(10,7), sharex=True, gridspec_kw={"hspace": 0.05})

    # (1) Mean deviations
    ax[0].bar(features - width/2, iqrerr_mean_kmed[:n_features], width=width,
            label="K-Medoids only", color="#1f77b4", alpha=0.7)
    ax[0].bar(features + width/2, iqrerr_mean_hyb[:n_features], width=width,
            label="Hybrid", color="#2ca02c", alpha=0.8)
    ax[0].set_ylabel("IQR-normalized |Δmean|")
    ax[0].legend(frameon=False, loc="upper left")
    ax[0].grid(axis="y", linestyle="--", alpha=0.3)
    ax[0].text(0.98, 0.90, f"Avg improvement: {mean_imp:+.1f}%",
            transform=ax[0].transAxes, ha="right", va="top",
            fontsize=10, color="#2ca02c")
    yt0 = [0.0, 0.1, 0.2]
    ax[0].set_yticks(yt0)
    ax[0].set_yticklabels([f"{y:.1f}" for y in yt0])
    ax[0].set_ylim(0,max(yt0))
    ax[0].set_xlim(0, n_features+0.5)

    # (2) 95th-percentile deviations
    ax[1].bar(features - width/2, iqrerr_q95_kmed[:n_features], width=width,
            label="K-Medoids only", color="#1f77b4", alpha=0.7)
    ax[1].bar(features + width/2, iqrerr_q95_hyb[:n_features], width=width,
            label="Hybrid", color="#2ca02c", alpha=0.8)
    ax[1].set_xlabel("Feature index", labelpad=10)
    ax[1].set_ylabel("IQR-normalized |Δ95th|")
    ax[1].grid(axis="y", linestyle="--", alpha=0.3)
    ax[1].text(0.98, 0.90, f"Avg improvement: {tail_imp:+.1f}%",
            transform=ax[1].transAxes, ha="right", va="top",
            fontsize=10,  color="#2ca02c")

    # --- Shared x-axis formatting ---
    yt1 = [0.0, 0.2, 0.4, 0.6]
    ax[1].set_xticks(features)
    ax[1].set_xticklabels(features)
    ax[1].set_xlim(0, n_features+0.5)
    ax[1].set_ylim(0,max(yt1))
    ax[1].set_yticks(yt1)
    ax[1].set_yticklabels([f"{y:.1f}" for y in yt1])

    # --- Remove top/right spines for both panels ---
    for a in ax:
        a.spines["top"].set_visible(False)
        a.spines["right"].set_visible(False)

    # --- Bottom title ---
    fig.align_ylabels(ax)
    fig.suptitle("Per-feature IQR-normalized deviations\n(K-Medoids only vs Hybrid)",
                y=0.02, fontsize=12, fontweight='bold')
  
    info = "[INFO 057]: > Save figure of per feature IQR comparison"
    savefigp(info, "057_Per_feature_IQR_comparison.png")

# ====================================================================================================

#       M  A  I  N      S  C  R  I  P  T

# ====================================================================================================

if __name__ == "__main__":
    
  
    # (1)  Generate initial scenario with halton --------------------------------------------------

    # 1.1 Generate Halton standard points
    halton = halton_generator(ndims=NDIMS, nsamples=NSAMPLES, sigma=SIGMA_0)

    # convert stored list-of-vectors to ndarray for downstream numeric ops
    halton_pts = np.asarray(list(halton['halton_pts'])) # shape 2D: (nsamples, ndims)
    halton_std_wgts = halton['halton_std_wgts']         # shape 1D: (nsamples,)
 
    # 1.2. plot Halton distributions
    plot_halton_distributions(halton)
 
    # 1.3. plot scaled Halton points
    plot_scaled_halton(halton, sigma=SIGMA_0)

    # 1.4. k-means clustering
    kmeans = kmeans_clustering(halton, N_SAMPLES=100, sigma0=SIGMA_0, ndim=100)
    
    # (2) --- K-Medoids Clustering ----------------------------------------------------------------
    
    # 2.1. K-Medoids anchors
    medoid_idx = medoid_anchors(halton, k_anchors=K_ANCHORS, seed=SEED)

    # (3) --- kmedoids + neural neigbour + hybrid -------------------------------------------------

    # 3.1 comprison between kmedoids, NN, and hybrid 
    scenario_pts, scenario_prb_wgts, refined_indices, max_dists, medoid_indices = weigthed_kmedoids_plus_NN_clustering(halton)

    # 3.2. plot refinement process
    plot_refinement_process(max_dists)

    # 3.2 Another NN assinment
    clusters, assign_medoid_idx = NN_assignment(halton, medoid_idx)

    # 3.3. plot cluster comparison
    plot_cluster_comparison(halton, scenario_pts, refined_indices, medoid_indices)

    # 3.4. cluster expansion
    selected = cluster_expansion(halton, medoid_idx, clusters, EXPAND_MODE, R_PER_CLUSTER, TAIL_QUANTILE)

    # 3.5. final reweighting and save
    representative_pts, representative_prb_wgts, D_sel = final_reweighting(halton, selected)
    print("[INFO 035]: Descriptif Statistics of scenario:")
    scenario_df = pd.DataFrame(representative_pts, columns=[f"feat_{j}" for j in range(representative_pts.shape[1])])
    scenario_df["prob_wgt"] = representative_prb_wgts
    scenario_df.describe().T


    # 3.6. Check & Diagnostics
    quick_diagnostics(selected, medoid_idx, D_sel)


    # (4)  Visualization of the selected representatives ------------------------------------------

    # 4.1. Another plot comparison
    plot_comparison(halton)

    # 4.2. Approximation error: anchors-only vs hybrid
    plot_approximation_error(halton, medoid_idx)

    # 4.3. Quantitative summary of approximation error reduction
    summary_df = summary_approximation_error(halton, medoid_idx)

    # 4.4. plot cluster probability mass
    plot_cluster_probability_mass(halton, medoid_idx)

    # 4.5. compute tail coverages
    D_anchor = pairwise_dist(halton_pts, halton_pts[medoid_idx])
    # pass medoid indices, selected set and representative weights to compute tail coverage
    tail_coverage = compute_tail_coverages(D_anchor, halton, medoid_idx, selected, representative_prb_wgts)

    # 4.6. compute tail error metrics
    df_overall, df_tail, tail_q, xq, w_anchor = compute_tail_error_metrics(halton, tail_coverage, representative_prb_wgts)
    
    # 4.7. Improvement summary (Hybrid vs K-Medoids only)
    improvement_summary(df_overall, df_tail)

    # (5) Visualization of improvements
    
    # 5.1. Visualisation ofPer-feature moments/quantiles: full vs hybri
    mu_full, mu_hyb, q95_full, q95_hyb = plot_per_feature_comparison(halton, selected, representative_prb_wgts)

    # 5.2. Quantitative summary of per-feature mean & tail preservation
    summary_df, df_feat = quantitative_per_feature_summary(mu_full, mu_hyb, q95_full, q95_hyb)

    # 5.3. Compare K-Medoids only and Hybrid performance per feature
    ensamble, error_metrics, summary_comp, df_comp = per_feature_comparison_kmed_vs_hyb(halton, medoid_idx, selected, representative_prb_wgts)

    # 5.4. Per-feature comparison: K-Medoids only vs Hybrid
    iqr_error, tbl, improvement = quantitative_per_feature_comparison_kmed_vs_hyb(ensamble)    

    # 5.5. Tail-guard selection
    tailguard = tail_guard_selection(halton, clusters, selected, representative_prb_wgts)

    # 5.6. Final save of updated scenario with tail-guard
    final_pts, final_wgts = apply_tail_guard(halton, selected, tailguard)

    # 5.7. Plot per-feature IQR-normalized error comparison
    plot_per_feature_iqr_comparison(iqr_error)
